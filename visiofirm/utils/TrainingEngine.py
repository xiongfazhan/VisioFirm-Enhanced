import os
import json
import yaml
import logging
import threading
import time
import os
from datetime import datetime
from pathlib import Path
import torch
from ultralytics import YOLO
from visiofirm.models.training import TrainingTask
from visiofirm.utils.performance_config import performance_manager
import shutil

# Configure logging with less verbose output
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# è®¾ç½®ultralyticsæ—¥å¿—çº§åˆ«ï¼Œå‡å°‘å†—ä½™è¾“å‡º
import logging
ultralytics_logger = logging.getLogger('ultralytics')
ultralytics_logger.setLevel(logging.ERROR)

# è®¾ç½®torchæ—¥å¿—çº§åˆ«
torch_logger = logging.getLogger('torch')
torch_logger.setLevel(logging.ERROR)

class TrainingEngine:
    def __init__(self, project_name, project_path):
        self.project_name = project_name
        self.project_path = project_path
        self.training_task = TrainingTask(project_name, project_path)
        self.current_task_id = None
        self.training_thread = None
        self.stop_training = False
        self._project = None

    @property
    def project(self):
        """å»¶è¿ŸåŠ è½½Projectå®ä¾‹ä»¥é¿å…å¾ªç¯å¯¼å…¥"""
        if self._project is None:
            from visiofirm.models.project import Project
            self._project = Project(self.project_name, "", "", self.project_path)
        return self._project

    def prepare_dataset(self, dataset_split):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        try:
            # éªŒè¯æ•°æ®é›†åˆ†å‰²æ¯”ä¾‹
            train_ratio = dataset_split.get('train', 0.7)
            val_ratio = dataset_split.get('val', 0.2)
            test_ratio = dataset_split.get('test', 0.1)
            
            total_ratio = train_ratio + val_ratio + test_ratio
            if abs(total_ratio - 1.0) > 0.01:
                raise ValueError(f"æ•°æ®é›†åˆ†å‰²æ¯”ä¾‹ä¹‹å’Œåº”ä¸º1.0ï¼Œå½“å‰ä¸º: {total_ratio}")
            
            # åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„
            dataset_dir = os.path.join(self.project_path, 'dataset')
            os.makedirs(dataset_dir, exist_ok=True)
            
            # åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ç›®å½•
            for split in ['train', 'val', 'test']:
                split_dir = os.path.join(dataset_dir, split)
                os.makedirs(os.path.join(split_dir, 'images'), exist_ok=True)
                os.makedirs(os.path.join(split_dir, 'labels'), exist_ok=True)

            # è·å–æ‰€æœ‰å·²æ ‡æ³¨çš„å›¾ç‰‡
            annotated_images = self.project.get_annotated_images()
            
            if not annotated_images:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°å·²æ ‡æ³¨çš„å›¾ç‰‡ï¼Œè¯·å…ˆå®Œæˆå›¾ç‰‡æ ‡æ³¨")
            
            logger.info(f"æ‰¾åˆ° {len(annotated_images)} å¼ å·²æ ‡æ³¨çš„å›¾ç‰‡")
            
            # éªŒè¯æ•°æ®é›†æœ€å°å¤§å°
            min_total_images = 2
            if len(annotated_images) < min_total_images:
                raise ValueError(f"æ•°æ®é›†å¤ªå°ï¼Œéœ€è¦è‡³å°‘ {min_total_images} å¼ å·²æ ‡æ³¨çš„å›¾ç‰‡ã€‚\n"
                               f"å½“å‰åªæœ‰ {len(annotated_images)} å¼ å·²æ ‡æ³¨å›¾ç‰‡ã€‚\n"
                               f"è¯·å…ˆå®Œæˆæ›´å¤šå›¾ç‰‡çš„æ ‡æ³¨åå†å¼€å§‹è®­ç»ƒã€‚")

            # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†
            total_images = len(annotated_images)
            
            # å¯¹äºå°æ•°æ®é›†ï¼Œè°ƒæ•´åˆ†å‰²ç­–ç•¥
            if total_images == 2:
                # åªæœ‰2å¼ å›¾ç‰‡æ—¶ï¼Œ1å¼ è®­ç»ƒï¼Œ1å¼ éªŒè¯
                train_count = 1
                val_count = 1
                test_count = 0
            elif total_images <= 5:
                # 3-5å¼ å›¾ç‰‡æ—¶ï¼Œé€‚å½“è°ƒæ•´æ¯”ä¾‹
                train_count = max(1, total_images - 1)
                val_count = 1
                test_count = 0
            else:
                # æ­£å¸¸æŒ‰æ¯”ä¾‹åˆ†å‰²
                train_count = max(1, int(total_images * train_ratio))
                val_count = max(1, int(total_images * val_ratio))
                test_count = total_images - train_count - val_count
            
            # åˆ†é…å›¾ç‰‡åˆ°ä¸åŒé›†åˆ
            train_images = annotated_images[:train_count]
            val_images = annotated_images[train_count:train_count + val_count]
            test_images = annotated_images[train_count + val_count:] if test_count > 0 else []

            # å¤åˆ¶å›¾ç‰‡å’Œæ ‡æ³¨æ–‡ä»¶
            self._copy_dataset_files(train_images, os.path.join(dataset_dir, 'train'))
            self._copy_dataset_files(val_images, os.path.join(dataset_dir, 'val'))
            if test_images:
                self._copy_dataset_files(test_images, os.path.join(dataset_dir, 'test'))

            # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
            self._create_dataset_yaml(dataset_dir)
            
            logger.info(f"æ•°æ®é›†å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†{len(train_images)}, éªŒè¯é›†{len(val_images)}, æµ‹è¯•é›†{len(test_images)}")
            return dataset_dir

        except Exception as e:
            logger.error(f"æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
            raise

    def _copy_dataset_files(self, images, target_dir):
        """å¤åˆ¶å›¾ç‰‡å’Œæ ‡æ³¨æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
        images_dir = os.path.join(target_dir, 'images')
        labels_dir = os.path.join(target_dir, 'labels')
        
        success_count = 0
        error_count = 0
        
        for image_info in images:
            try:
                image_name = image_info['name']
                image_path = image_info['path']
                
                # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(image_path):
                    logger.warning(f"æºå›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                    error_count += 1
                    continue
                
                # å¤åˆ¶å›¾ç‰‡
                target_image_path = os.path.join(images_dir, image_name)
                shutil.copy2(image_path, target_image_path)
                
                # ç”ŸæˆYOLOæ ¼å¼æ ‡æ³¨æ–‡ä»¶
                label_name = os.path.splitext(image_name)[0] + '.txt'
                label_path = os.path.join(labels_dir, label_name)
                self._generate_yolo_label(image_info, label_path)
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥ {image_info.get('name', 'unknown')}: {e}")
                error_count += 1
        
        logger.info(f"æ–‡ä»¶å¤åˆ¶å®Œæˆ: æˆåŠŸ{success_count}, å¤±è´¥{error_count}")
        
        if success_count == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸå¤åˆ¶ä»»ä½•æ–‡ä»¶")

    def _generate_yolo_label(self, image_info, label_path):
        """ç”ŸæˆYOLOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶"""
        try:
            annotations = self.project.get_annotations_by_image_id(image_info['id'])
            classes = self.project.get_classes()
            class_map = {cls: idx for idx, cls in enumerate(classes)}
            
            with open(label_path, 'w') as f:
                for ann in annotations:
                    if ann['class_name'] in class_map:
                        class_id = class_map[ann['class_name']]
                        
                        # è½¬æ¢åæ ‡ä¸ºYOLOæ ¼å¼ (ç›¸å¯¹åæ ‡)
                        img_width = image_info['width']
                        img_height = image_info['height']
                        
                        if ann['type'] in ['rect', 'bbox'] and ann['x'] is not None:
                            # è¾¹ç•Œæ¡†æ ¼å¼: class_id center_x center_y width height
                            x, y, w, h = ann['x'], ann['y'], ann['width'], ann['height']
                            center_x = (x + w/2) / img_width
                            center_y = (y + h/2) / img_height
                            norm_w = w / img_width
                            norm_h = h / img_height
                            
                            f.write(f"{class_id} {center_x:.6f} {center_y:.6f} {norm_w:.6f} {norm_h:.6f}\n")
                        
                        elif ann['type'] == 'polygon' and ann.get('points'):
                            # åˆ†å‰²æ ¼å¼: class_id x1 y1 x2 y2 ... xn yn
                            points = ann['points']
                            normalized_points = []
                            for point in points:
                                norm_x = point['x'] / img_width
                                norm_y = point['y'] / img_height
                                normalized_points.extend([f"{norm_x:.6f}", f"{norm_y:.6f}"])
                            
                            f.write(f"{class_id} {' '.join(normalized_points)}\n")
                            
        except Exception as e:
            logger.error(f"ç”ŸæˆYOLOæ ‡æ³¨æ–‡ä»¶å¤±è´¥: {e}")

    def _create_dataset_yaml(self, dataset_dir):
        """åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶"""
        try:
            classes = self.project.get_classes()
            # ç¡®ä¿ classes æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            if classes and isinstance(classes[0], dict):
                class_names = [cls['name'] for cls in classes]
            else:
                class_names = classes if classes else []
            
            dataset_config = {
                'path': dataset_dir,
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'nc': len(class_names),
                'names': class_names
            }
            
            yaml_path = os.path.join(dataset_dir, 'dataset.yaml')
            with open(yaml_path, 'w', encoding='utf-8') as f:
                yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)
                
            logger.info(f"æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {yaml_path}")
            return yaml_path
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def start_training(self, task_id, model_type, dataset_split, config):
        """å¼€å§‹è®­ç»ƒä»»åŠ¡"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯åŠ¨æ–°ä»»åŠ¡
            can_start, message = performance_manager.can_start_task(task_id)
            if not can_start:
                logger.warning(f"æ— æ³•å¯åŠ¨ä»»åŠ¡ {task_id}: {message}")
                self.training_task.update_task_status(task_id, 'failed', 0, message)
                return False
            
            self.current_task_id = task_id
            self.stop_training = False
            
            # è·å–ä¼˜åŒ–çš„è®­ç»ƒé…ç½®
            optimized_config = performance_manager.get_memory_efficient_config(
                model_type, 
                config.get('epochs', 100), 
                config.get('device', 'auto')
            )
            
            # åˆå¹¶ç”¨æˆ·é…ç½®å’Œä¼˜åŒ–é…ç½®
            final_config = {**optimized_config, **config}
            
            # æ³¨å†Œä»»åŠ¡åˆ°æ€§èƒ½ç®¡ç†å™¨
            task_info = {
                'start_time': datetime.now(),
                'model_type': model_type,
                'device': final_config.get('device', 'auto'),
                'batch_size': final_config.get('batch', 16)
            }
            performance_manager.register_task(task_id, task_info)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            self.training_task.update_task_status(task_id, 'running', 0)
            
            # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè®­ç»ƒ
            self.training_thread = threading.Thread(
                target=self._run_training,
                args=(task_id, model_type, dataset_split, final_config)
            )
            self.training_thread.start()
            
            logger.info(f"è®­ç»ƒä»»åŠ¡ {task_id} å·²å¯åŠ¨ï¼Œä½¿ç”¨ä¼˜åŒ–é…ç½®")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨è®­ç»ƒå¤±è´¥: {e}")
            performance_manager.unregister_task(task_id)
            self.training_task.update_task_status(task_id, 'failed', 0, str(e))
            return False

    def _get_optimal_device(self, requested_device='auto'):
        """è·å–æœ€ä¼˜çš„è®­ç»ƒè®¾å¤‡"""
        if requested_device != 'auto':
            return requested_device
        
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            optimal_device = 'cuda:0'
            logger.info(f"æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨: {optimal_device}")
            return optimal_device
        else:
            logger.info("æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
            return 'cpu'

    def _run_training(self, task_id, model_type, dataset_split, config):
        """æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹"""
        try:
            # å‡†å¤‡æ•°æ®é›†
            dataset_dir = self.prepare_dataset(dataset_split)
            dataset_yaml = os.path.join(dataset_dir, 'dataset.yaml')
            
            # ç¡®ä¿æ¨¡å‹å¯ç”¨
            if model_type.startswith('yolo'):
                model_path = self.ensure_model_available(model_type)
                model = YOLO(model_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            
            # è·å–æœ€ä¼˜è®¾å¤‡
            optimal_device = self._get_optimal_device(config.get('device', 'auto'))
            
            # è®¾ç½®è®­ç»ƒå‚æ•°
            train_args = {
                'data': dataset_yaml,
                'epochs': config.get('epochs', 100),
                'batch': config.get('batch_size', 16),
                'imgsz': config.get('image_size', 640),
                'lr0': config.get('learning_rate', 0.01),
                'device': optimal_device,
                'project': os.path.join(self.project_path, 'training_runs'),
                'name': f"task_{task_id}",
                'save_period': 10,  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
                'patience': 50,     # æ—©åœè€å¿ƒå€¼
                'verbose': True
            }
            
            # æ·»åŠ å…¶ä»–å‚æ•°
            if 'optimizer' in config and config['optimizer'] != 'auto':
                train_args['optimizer'] = config['optimizer']
            
            # è‡ªå®šä¹‰å›è°ƒå‡½æ•°æ¥ç›‘æ§è®­ç»ƒè¿›åº¦
            def on_train_epoch_end(trainer):
                try:
                    if self.stop_training:
                        trainer.stop = True
                        return
                    
                    epoch = trainer.epoch + 1  # YOLOçš„epochä»0å¼€å§‹ï¼Œæ˜¾ç¤ºæ—¶+1
                    total_epochs = trainer.epochs
                    progress = int((epoch / total_epochs) * 100)
                    
                    # æ›´æ–°è¿›åº¦
                    self.training_task.update_task_status(task_id, 'running', progress)
                    
                    # è®°å½•è®­ç»ƒæ—¥å¿—
                    try:
                        # ä»è®­ç»ƒå™¨ä¸­æå–æŸå¤±å€¼
                        loss = None
                        if hasattr(trainer, 'loss') and trainer.loss is not None:
                            loss = float(trainer.loss.item()) if hasattr(trainer.loss, 'item') else float(trainer.loss)
                        elif hasattr(trainer, 'losses') and trainer.losses:
                            loss = float(trainer.losses[-1]) if trainer.losses else None
                        elif hasattr(trainer, 'tloss') and trainer.tloss is not None:
                            loss = float(trainer.tloss)
                        
                        # è®°å½•è®­ç»ƒè¿›åº¦åˆ°æ•°æ®åº“
                        if loss is not None:
                            self.training_task.log_training_progress(task_id, epoch, loss)
                            # åªåœ¨æ¯5ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ—¶è¾“å‡ºæ—¥å¿—
                            if epoch % 5 == 0 or epoch == total_epochs:
                                logger.info(f"ğŸ“ˆ Epoch {epoch}/{total_epochs} | Loss: {loss:.4f} | Progress: {progress}%")
                        else:
                            # åªåœ¨æ¯5ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ—¶è¾“å‡ºæ—¥å¿—
                            if epoch % 5 == 0 or epoch == total_epochs:
                                logger.info(f"ğŸ“ˆ Epoch {epoch}/{total_epochs} | Progress: {progress}%")
                            
                    except Exception as log_error:
                        logger.warning(f"è®°å½•è®­ç»ƒæ—¥å¿—æ—¶å‡ºé”™: {log_error}")
                        
                except Exception as callback_error:
                    logger.error(f"è®­ç»ƒå›è°ƒå‡½æ•°å‡ºé”™: {callback_error}")
            
            def on_train_end(trainer):
                try:
                    # è®­ç»ƒç»“æŸæ—¶çš„å¤„ç†
                    logger.info(f"âœ… è®­ç»ƒä»»åŠ¡ {task_id} å®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ è®­ç»ƒç»“æŸå›è°ƒå‡ºé”™: {e}")
            
            # æ·»åŠ å›è°ƒ
            model.add_callback('on_train_epoch_end', on_train_epoch_end)
            model.add_callback('on_train_end', on_train_end)
            
            # å¼€å§‹è®­ç»ƒ
            logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒä»»åŠ¡ {task_id}")
            logger.info(f"ğŸ“Š æ¨¡å‹: {model_type} | è®¾å¤‡: {optimal_device} | è½®æ•°: {config.get('epochs', 100)}")
            
            # ä¸´æ—¶é‡å®šå‘stdoutæ¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†—ä½™è¾“å‡º
            import sys
            from io import StringIO
            old_stdout = sys.stdout
            sys.stdout = StringIO()
            
            try:
                results = model.train(**train_args)
            finally:
                # æ¢å¤stdout
                sys.stdout = old_stdout
            
            if not self.stop_training:
                # è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹è·¯å¾„å’ŒæŒ‡æ ‡
                weights_dir = os.path.join(train_args['project'], train_args['name'], 'weights')
                best_model_path = os.path.join(weights_dir, 'best.pt')
                
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
                if not os.path.exists(best_model_path):
                    # å¦‚æœ best.pt ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ last.pt
                    last_model_path = os.path.join(weights_dir, 'last.pt')
                    if os.path.exists(last_model_path):
                        best_model_path = last_model_path
                        logger.warning(f"âš ï¸ best.pt ä¸å­˜åœ¨ï¼Œä½¿ç”¨ last.pt: {best_model_path}")
                    else:
                        logger.error(f"âŒ è®­ç»ƒå®Œæˆä½†æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {weights_dir}")
                        self.training_task.update_task_status(task_id, 'failed', None, "è®­ç»ƒå®Œæˆä½†æ¨¡å‹æ–‡ä»¶ä¸¢å¤±")
                        return
                
                # æå–è®­ç»ƒæŒ‡æ ‡
                metrics = {}
                try:
                    if hasattr(results, 'results_dict'):
                        metrics = results.results_dict
                    elif hasattr(results, 'maps') and results.maps:
                        metrics = {'mAP50': float(results.maps[0])}
                    elif hasattr(results, 'box') and hasattr(results.box, 'map50'):
                        metrics = {
                            'mAP50': float(results.box.map50),
                            'mAP50-95': float(results.box.map) if hasattr(results.box, 'map') else 0.0
                        }
                    
                    # æ·»åŠ æ›´å¤šæŒ‡æ ‡
                    if hasattr(results, 'box'):
                        if hasattr(results.box, 'mp'):
                            metrics['precision'] = float(results.box.mp)
                        if hasattr(results.box, 'mr'):
                            metrics['recall'] = float(results.box.mr)
                            
                except Exception as metrics_error:
                    logger.warning(f"âš ï¸ æå–è®­ç»ƒæŒ‡æ ‡æ—¶å‡ºé”™: {metrics_error}")
                    metrics = {'note': 'è®­ç»ƒå®Œæˆä½†æŒ‡æ ‡æå–å¤±è´¥'}
                
                self.training_task.update_task_status(
                    task_id, 'completed', 100, 
                    model_path=best_model_path, 
                    metrics=metrics
                )
                
                logger.info(f"ğŸ¯ è®­ç»ƒä»»åŠ¡ {task_id} å®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {best_model_path}")
            else:
                self.training_task.update_task_status(task_id, 'stopped', None, "è®­ç»ƒè¢«ç”¨æˆ·åœæ­¢")
                logger.info(f"â¹ï¸ è®­ç»ƒä»»åŠ¡ {task_id} è¢«åœæ­¢")
                
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
            self.training_task.update_task_status(task_id, 'failed', None, str(e))
        finally:
            # ç¡®ä¿ä»»åŠ¡åœ¨ç»“æŸæ—¶è¢«æ³¨é”€
            performance_manager.unregister_task(task_id)

    def _check_and_fix_task_status(self, task_id):
        """æ£€æŸ¥å¹¶ä¿®å¤ä»»åŠ¡çŠ¶æ€ä¸ä¸€è‡´é—®é¢˜"""
        try:
            db_task = self.training_task.get_task_details(task_id)
            if not db_task:
                return None, "ä»»åŠ¡ä¸å­˜åœ¨"
            
            # æ£€æŸ¥çŠ¶æ€ä¸€è‡´æ€§
            db_running = db_task['status'] == 'running'
            thread_running = (self.current_task_id == task_id and 
                             self.training_thread and 
                             self.training_thread.is_alive())
            
            # çŠ¶æ€ä¸ä¸€è‡´æ—¶è‡ªåŠ¨ä¿®å¤
            if db_running and not thread_running:
                logger.warning(f"æ£€æµ‹åˆ°çŠ¶æ€ä¸ä¸€è‡´ï¼Œè‡ªåŠ¨ä¿®å¤ä»»åŠ¡ {task_id}")
                self.training_task.update_task_status(task_id, 'failed', None, "è®­ç»ƒè¿›ç¨‹å¼‚å¸¸ç»“æŸ")
                performance_manager.unregister_task(task_id)  # æ¸…ç†æ€§èƒ½ç®¡ç†å™¨
                return 'fixed', "çŠ¶æ€å·²è‡ªåŠ¨ä¿®å¤"
            
            return db_task['status'], None
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return None, str(e)

    def stop_training_task(self, task_id):
        """åœæ­¢è®­ç»ƒä»»åŠ¡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            # æ£€æŸ¥å¹¶ä¿®å¤çŠ¶æ€
            status, message = self._check_and_fix_task_status(task_id)
            
            if status is None:
                logger.error(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨æˆ–æ£€æŸ¥å¤±è´¥: {message}")
                return False
            
            if status == 'fixed':
                logger.info(f"ä»»åŠ¡ {task_id} çŠ¶æ€å·²ä¿®å¤: {message}")
                return True
            
            if status != 'running':
                logger.warning(f"ä»»åŠ¡ {task_id} å½“å‰çŠ¶æ€ä¸º {status}ï¼Œæ— éœ€åœæ­¢")
                return False
            
            # æ­£å¸¸åœæ­¢æµç¨‹
            if self.current_task_id == task_id and self.training_thread and self.training_thread.is_alive():
                self.stop_training = True
                self.training_thread.join(timeout=30)  # ç­‰å¾…æœ€å¤š30ç§’
                
                # ä»æ€§èƒ½ç®¡ç†å™¨ä¸­æ³¨é”€ä»»åŠ¡
                performance_manager.unregister_task(task_id)
                
                self.training_task.update_task_status(task_id, 'stopped', None, "è®­ç»ƒè¢«ç”¨æˆ·åœæ­¢")
                logger.info(f"è®­ç»ƒä»»åŠ¡ {task_id} å·²åœæ­¢")
                return True
            else:
                # å¼ºåˆ¶çŠ¶æ€ä¿®å¤ - æ•°æ®åº“æ˜¾ç¤ºè¿è¡Œä½†çº¿ç¨‹å·²ç»“æŸ
                logger.warning(f"å¼ºåˆ¶ä¿®å¤ä»»åŠ¡ {task_id} çŠ¶æ€")
                self.training_task.update_task_status(task_id, 'stopped', None, "å¼ºåˆ¶åœæ­¢")
                performance_manager.unregister_task(task_id)
                return True
                
        except Exception as e:
            logger.error(f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {e}")
            performance_manager.unregister_task(task_id)  # ç¡®ä¿æ¸…ç†
            return False

    def get_training_status(self, task_id):
        """è·å–è®­ç»ƒçŠ¶æ€"""
        return self.training_task.get_task_details(task_id)

    def get_available_models(self):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        models = [
            {'name': 'yolov8n', 'description': 'YOLOv8 Nano - æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½', 'size': '6MB'},
            {'name': 'yolov8s', 'description': 'YOLOv8 Small - å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦', 'size': '22MB'},
            {'name': 'yolov8m', 'description': 'YOLOv8 Medium - è¾ƒé«˜ç²¾åº¦', 'size': '50MB'},
            {'name': 'yolov8l', 'description': 'YOLOv8 Large - é«˜ç²¾åº¦', 'size': '87MB'},
            {'name': 'yolov8x', 'description': 'YOLOv8 Extra Large - æœ€é«˜ç²¾åº¦', 'size': '136MB'},
            {'name': 'yolov10n', 'description': 'YOLOv10 Nano - æœ€æ–°ç‰ˆæœ¬ï¼Œæœ€å¿«', 'size': '5MB'},
            {'name': 'yolov10s', 'description': 'YOLOv10 Small - æœ€æ–°ç‰ˆæœ¬ï¼Œå¹³è¡¡', 'size': '20MB'},
            {'name': 'yolov10m', 'description': 'YOLOv10 Medium - æœ€æ–°ç‰ˆæœ¬ï¼Œè¾ƒé«˜ç²¾åº¦', 'size': '45MB'},
            {'name': 'yolov10l', 'description': 'YOLOv10 Large - æœ€æ–°ç‰ˆæœ¬ï¼Œé«˜ç²¾åº¦', 'size': '80MB'},
            {'name': 'yolov10x', 'description': 'YOLOv10 Extra Large - æœ€æ–°ç‰ˆæœ¬ï¼Œæœ€é«˜ç²¾åº¦', 'size': '125MB'}
        ]
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
        for model in models:
            model_path = self._get_model_path(model['name'])
            model['downloaded'] = os.path.exists(model_path)
            model['path'] = model_path
        
        return models

    def _get_model_path(self, model_name):
        """è·å–æ¨¡å‹æ–‡ä»¶è·¯å¾„"""
        from visiofirm.config import WEIGHTS_FOLDER
        return os.path.join(WEIGHTS_FOLDER, f"{model_name}.pt")

    def download_model(self, model_name, progress_callback=None):
        """ä¸‹è½½æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            model_path = self._get_model_path(model_name)
            
            # å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
            if os.path.exists(model_path):
                logger.info(f"æ¨¡å‹ {model_name} å·²å­˜åœ¨: {model_path}")
                return model_path
            
            # ç¡®ä¿æƒé‡ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
            
            # ä½¿ç”¨YOLOè‡ªåŠ¨ä¸‹è½½åŠŸèƒ½ï¼Œä½†ç¦ç”¨è¿›åº¦æ¡
            import os
            import sys
            from io import StringIO
            
            # ä¸´æ—¶é‡å®šå‘stdoutæ¥éšè—YOLOçš„ä¸‹è½½è¿›åº¦
            old_stdout = sys.stdout
            sys.stdout = StringIO()
            
            try:
                model = YOLO(f"{model_name}.pt")
                
                # å°†ä¸‹è½½çš„æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šä½ç½®
                if hasattr(model, 'ckpt_path') and os.path.exists(model.ckpt_path):
                    shutil.move(model.ckpt_path, model_path)
                    logger.info(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ: {model_path}")
                else:
                    # å¦‚æœYOLOæ²¡æœ‰è‡ªåŠ¨ä¸‹è½½ï¼Œæ‰‹åŠ¨ä¸‹è½½
                    import requests
                    
                    # YOLOå®˜æ–¹æ¨¡å‹ä¸‹è½½URL
                    model_urls = {
                        'yolov8n': 'https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt',
                        'yolov8s': 'https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt',
                        'yolov8m': 'https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt',
                        'yolov8l': 'https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt',
                        'yolov8x': 'https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt',
                        'yolov10n': 'https://github.com/ultralytics/assets/releases/download/v0.1.0/yolo10n.pt',
                        'yolov10s': 'https://github.com/ultralytics/assets/releases/download/v0.1.0/yolo10s.pt',
                        'yolov10m': 'https://github.com/ultralytics/assets/releases/download/v0.1.0/yolo10m.pt',
                        'yolov10l': 'https://github.com/ultralytics/assets/releases/download/v0.1.0/yolo10l.pt',
                        'yolov10x': 'https://github.com/ultralytics/assets/releases/download/v0.1.0/yolo10x.pt'
                    }
                    
                    if model_name in model_urls:
                        url = model_urls[model_name]
                        logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹ {model_name}...")
                        response = requests.get(url, stream=True)
                        response.raise_for_status()
                        
                        total_size = int(response.headers.get('content-length', 0))
                        downloaded = 0
                        
                        with open(model_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                if chunk:
                                    f.write(chunk)
                                    downloaded += len(chunk)
                                    if progress_callback:
                                        progress_callback(downloaded, total_size)
                        
                        logger.info(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ ({downloaded/1024/1024:.1f}MB)")
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            finally:
                # æ¢å¤stdout
                sys.stdout = old_stdout
            
            logger.info(f"æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
            return model_path
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
            raise

    def ensure_model_available(self, model_name):
        """ç¡®ä¿æ¨¡å‹å¯ç”¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½"""
        model_path = self._get_model_path(model_name)
        
        if not os.path.exists(model_path):
            logger.info(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨ï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½...")
            return self.download_model(model_name)
        
        return model_path

    def get_device_info(self):
        """è·å–å¯ç”¨çš„è®¡ç®—è®¾å¤‡ä¿¡æ¯"""
        devices = [{'name': 'auto', 'description': 'è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡'}]
        
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                device_name = torch.cuda.get_device_name(i)
                devices.append({
                    'name': f'cuda:{i}',
                    'description': f'GPU {i}: {device_name}'
                })
        
        # æ·»åŠ CPUé€‰é¡¹
        devices.append({'name': 'cpu', 'description': 'CPU (è¾ƒæ…¢ä½†å…¼å®¹æ€§å¥½)'})
        
        return devices

    def export_model(self, model_path, export_format='onnx', **kwargs):
        """å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹"""
        supported_formats = {
            'onnx': {
                'extension': '.onnx',
                'description': 'ONNX - è·¨å¹³å°æ¨ç†æ ¼å¼'
            },
            'torchscript': {
                'extension': '.torchscript',
                'description': 'TorchScript - PyTorchä¼˜åŒ–æ ¼å¼'
            },
            'tflite': {
                'extension': '.tflite',
                'description': 'TensorFlow Lite - ç§»åŠ¨ç«¯éƒ¨ç½²'
            },
            'coreml': {
                'extension': '.mlmodel',
                'description': 'Core ML - iOS/macOSéƒ¨ç½²'
            },
            'tensorrt': {
                'extension': '.engine',
                'description': 'TensorRT - NVIDIA GPUåŠ é€Ÿ'
            }
        }
        
        try:
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            if export_format not in supported_formats:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {export_format}. æ”¯æŒçš„æ ¼å¼: {list(supported_formats.keys())}")
            
            logger.info(f"å¼€å§‹å¯¼å‡ºæ¨¡å‹ä¸º {export_format} æ ¼å¼...")
            
            model = YOLO(model_path)
            
            # è®¾ç½®å¯¼å‡ºå‚æ•°
            export_kwargs = {
                'format': export_format,
                'half': kwargs.get('half', False),  # FP16é‡åŒ–
                'int8': kwargs.get('int8', False),  # INT8é‡åŒ–
                'dynamic': kwargs.get('dynamic', False),  # åŠ¨æ€è¾“å…¥å°ºå¯¸
                'simplify': kwargs.get('simplify', True),  # ç®€åŒ–ONNXæ¨¡å‹
                'opset': kwargs.get('opset', 12),  # ONNX opsetç‰ˆæœ¬
                'workspace': kwargs.get('workspace', 4),  # TensorRTå·¥ä½œç©ºé—´(GB)
                'batch': kwargs.get('batch', 1),  # æ‰¹å¤„ç†å¤§å°
                'device': kwargs.get('device', None),  # è®¾å¤‡
            }
            
            # ç§»é™¤Noneå€¼å‚æ•°
            export_kwargs = {k: v for k, v in export_kwargs.items() if v is not None}
            
            export_path = model.export(**export_kwargs)
            
            if not os.path.exists(export_path):
                raise RuntimeError(f"å¯¼å‡ºå¤±è´¥ï¼Œæ–‡ä»¶æœªç”Ÿæˆ: {export_path}")
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(export_path) / (1024 * 1024)  # MB
            
            logger.info(f"æ¨¡å‹å·²æˆåŠŸå¯¼å‡ºä¸º {export_format} æ ¼å¼")
            logger.info(f"å¯¼å‡ºè·¯å¾„: {export_path}")
            logger.info(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            
            return {
                'success': True,
                'export_path': export_path,
                'format': export_format,
                'file_size_mb': round(file_size, 2),
                'description': supported_formats[export_format]['description']
            }
            
        except Exception as e:
            logger.error(f"æ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'format': export_format
            }

    def get_resource_usage(self):
        """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        return performance_manager.monitor_resources()

    def get_performance_suggestions(self, model_type, current_config):
        """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        resource_usage = self.get_resource_usage()
        return performance_manager.suggest_optimization(model_type, current_config, resource_usage)

    def get_optimal_config(self, model_type, epochs, device='auto'):
        """è·å–ä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
        return performance_manager.get_memory_efficient_config(model_type, epochs, device)
    def validate_model(self, model_path, dataset_yaml, **kwargs):
        """éªŒè¯è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            if not os.path.exists(dataset_yaml):
                raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_yaml}")
            
            logger.info(f"å¼€å§‹éªŒè¯æ¨¡å‹: {model_path}")
            
            model = YOLO(model_path)
            
            # è®¾ç½®éªŒè¯å‚æ•°
            val_kwargs = {
                'data': dataset_yaml,
                'imgsz': kwargs.get('imgsz', 640),  # å›¾åƒå°ºå¯¸
                'batch': kwargs.get('batch', 16),   # æ‰¹å¤„ç†å¤§å°
                'conf': kwargs.get('conf', 0.001),  # ç½®ä¿¡åº¦é˜ˆå€¼
                'iou': kwargs.get('iou', 0.6),      # IoUé˜ˆå€¼
                'device': kwargs.get('device', None), # è®¾å¤‡
                'half': kwargs.get('half', False),   # FP16æ¨ç†
                'save_json': kwargs.get('save_json', True),  # ä¿å­˜JSONç»“æœ
                'save_hybrid': kwargs.get('save_hybrid', False), # ä¿å­˜æ··åˆæ ‡ç­¾
                'plots': kwargs.get('plots', True),  # ç”Ÿæˆå›¾è¡¨
                'verbose': kwargs.get('verbose', True) # è¯¦ç»†è¾“å‡º
            }
            
            # ç§»é™¤Noneå€¼å‚æ•°
            val_kwargs = {k: v for k, v in val_kwargs.items() if v is not None}
            
            results = model.val(**val_kwargs)
            
            # æå–éªŒè¯æŒ‡æ ‡
            metrics = {}
            
            if hasattr(results, 'box') and results.box:
                box_metrics = results.box
                
                # åŸºæœ¬æŒ‡æ ‡
                metrics.update({
                    'mAP50': float(box_metrics.map50) if hasattr(box_metrics, 'map50') else 0.0,
                    'mAP50-95': float(box_metrics.map) if hasattr(box_metrics, 'map') else 0.0,
                    'precision': float(box_metrics.mp) if hasattr(box_metrics, 'mp') else 0.0,
                    'recall': float(box_metrics.mr) if hasattr(box_metrics, 'mr') else 0.0,
                    'f1_score': 0.0
                })
                
                # è®¡ç®—F1åˆ†æ•°
                if metrics['precision'] > 0 and metrics['recall'] > 0:
                    metrics['f1_score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])
                
                # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
                if hasattr(box_metrics, 'ap') and hasattr(box_metrics, 'ap50'):
                    try:
                        # è¯»å–æ•°æ®é›†é…ç½®è·å–ç±»åˆ«åç§°
                        with open(dataset_yaml, 'r', encoding='utf-8') as f:
                            dataset_config = yaml.safe_load(f)
                            class_names = dataset_config.get('names', [])
                        
                        if class_names and hasattr(box_metrics, 'ap50'):
                            class_metrics = {}
                            ap50_values = box_metrics.ap50 if hasattr(box_metrics.ap50, '__iter__') else [box_metrics.ap50]
                            
                            for i, class_name in enumerate(class_names):
                                if i < len(ap50_values):
                                    class_metrics[class_name] = {
                                        'AP50': float(ap50_values[i]) if ap50_values[i] is not None else 0.0
                                    }
                            
                            metrics['class_metrics'] = class_metrics
                            
                    except Exception as e:
                        logger.warning(f"æ— æ³•è·å–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡: {e}")
            
            # æ·»åŠ éªŒè¯ä¿¡æ¯
            metrics.update({
                'validation_date': datetime.now().isoformat(),
                'model_path': model_path,
                'dataset_yaml': dataset_yaml,
                'validation_params': val_kwargs
            })
            
            logger.info(f"æ¨¡å‹éªŒè¯å®Œæˆ")
            logger.info(f"mAP50: {metrics.get('mAP50', 0):.4f}, mAP50-95: {metrics.get('mAP50-95', 0):.4f}")
            logger.info(f"Precision: {metrics.get('precision', 0):.4f}, Recall: {metrics.get('recall', 0):.4f}")
            
            return {
                'success': True,
                'metrics': metrics,
                'results_path': getattr(results, 'save_dir', None)
            }
            
        except Exception as e:
            logger.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }