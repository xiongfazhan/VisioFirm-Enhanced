#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†æ¨¡å‹æµ‹è¯•æ¨¡å—
æµ‹è¯•æ•°æ®é›†æ¨¡å‹çš„CRUDæ“ä½œå’Œæ•°æ®åº“äº¤äº’åŠŸèƒ½
"""

import unittest
import tempfile
import os
import shutil
import sqlite3
from datetime import datetime
import sys
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from visiofirm.models.dataset import (
    Dataset, 
    init_dataset_db, 
    create_dataset, 
    get_dataset_by_id,
    get_datasets,
    update_dataset,
    delete_dataset,
    search_datasets,
    get_dataset_classes,
    add_dataset_classes,
    link_dataset_to_project,
    unlink_dataset_from_project,
    get_project_datasets
)


class TestDatasetModel(unittest.TestCase):
    """æ•°æ®é›†æ¨¡å‹æµ‹è¯•ç±»"""
    
    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»è®¾ç½®"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        cls.temp_dir = tempfile.mkdtemp()
        cls.test_db_path = os.path.join(cls.temp_dir, 'test_datasets.db')
        
        # åˆå§‹åŒ–æµ‹è¯•æ•°æ®åº“
        cls.setup_test_database()
        
    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(cls.temp_dir):
            shutil.rmtree(cls.temp_dir)
    
    @classmethod
    def setup_test_database(cls):
        """è®¾ç½®æµ‹è¯•æ•°æ®åº“"""
        # åˆ›å»ºæ•°æ®é›†è¡¨
        init_dataset_db(cls.test_db_path)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†æ–‡ä»¶
        cls.create_test_dataset_files()
    
    @classmethod
    def create_test_dataset_files(cls):
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†æ–‡ä»¶"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ç›®å½•
        cls.test_dataset_dir = os.path.join(cls.temp_dir, 'test_dataset')
        os.makedirs(cls.test_dataset_dir, exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯•å›¾ç‰‡ç›®å½•
        images_dir = os.path.join(cls.test_dataset_dir, 'images')
        os.makedirs(images_dir, exist_ok=True)
        
        # åˆ›å»ºä¸€äº›æµ‹è¯•å›¾ç‰‡æ–‡ä»¶ï¼ˆç©ºæ–‡ä»¶ï¼‰
        for i in range(5):
            with open(os.path.join(images_dir, f'test_image_{i}.jpg'), 'w') as f:
                f.write('')
        
        # åˆ›å»ºæ ‡æ³¨æ–‡ä»¶
        annotations_dir = os.path.join(cls.test_dataset_dir, 'annotations')
        os.makedirs(annotations_dir, exist_ok=True)
        
        # åˆ›å»ºç±»åˆ«æ–‡ä»¶
        with open(os.path.join(cls.test_dataset_dir, 'classes.txt'), 'w') as f:
            f.write('cat\ndog\nbird\n')
    
    def setUp(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•çš„è®¾ç½®"""
        # æ¸…ç†æ•°æ®åº“æ•°æ®
        with sqlite3.connect(self.test_db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('DELETE FROM Project_Datasets')
            cursor.execute('DELETE FROM Dataset_Classes')
            cursor.execute('DELETE FROM Datasets')
            conn.commit()
    
    def test_dataset_class_creation(self):
        """æµ‹è¯•Datasetç±»çš„åˆ›å»º"""
        dataset = Dataset(
            name="Test Dataset",
            description="A test dataset",
            dataset_type="custom",
            file_path=self.test_dataset_dir
        )
        
        self.assertEqual(dataset.name, "Test Dataset")
        self.assertEqual(dataset.description, "A test dataset")
        self.assertEqual(dataset.dataset_type, "custom")
        self.assertEqual(dataset.file_path, self.test_dataset_dir)
        self.assertIsNone(dataset.dataset_id)
    
    def test_create_dataset(self):
        """æµ‹è¯•åˆ›å»ºæ•°æ®é›†"""
        dataset_data = {
            'name': 'Test Dataset 1',
            'description': 'First test dataset',
            'dataset_type': 'custom',
            'file_path': self.test_dataset_dir,
            'file_size': 1024000,
            'image_count': 5,
            'class_count': 3,
            'annotation_format': 'yolo'
        }
        
        dataset_id = create_dataset(self.test_db_path, **dataset_data)
        self.assertIsNotNone(dataset_id)
        self.assertIsInstance(dataset_id, int)
        
        # éªŒè¯æ•°æ®é›†å·²åˆ›å»º
        dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertIsNotNone(dataset)
        self.assertEqual(dataset.name, 'Test Dataset 1')
        self.assertEqual(dataset.description, 'First test dataset')
        self.assertEqual(dataset.dataset_type, 'custom')
        self.assertEqual(dataset.file_size, 1024000)
        self.assertEqual(dataset.image_count, 5)
        self.assertEqual(dataset.class_count, 3)
        self.assertEqual(dataset.annotation_format, 'yolo')
    
    def test_get_dataset_by_id(self):
        """æµ‹è¯•æ ¹æ®IDè·å–æ•°æ®é›†"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset_id = create_dataset(
            self.test_db_path,
            name='Test Dataset 2',
            description='Second test dataset',
            dataset_type='downloaded',
            file_path=self.test_dataset_dir
        )
        
        # è·å–æ•°æ®é›†
        dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertIsNotNone(dataset)
        self.assertEqual(dataset.dataset_id, dataset_id)
        self.assertEqual(dataset.name, 'Test Dataset 2')
        
        # æµ‹è¯•ä¸å­˜åœ¨çš„ID
        nonexistent_dataset = get_dataset_by_id(self.test_db_path, 99999)
        self.assertIsNone(nonexistent_dataset)
    
    def test_get_all_datasets(self):
        """æµ‹è¯•è·å–æ‰€æœ‰æ•°æ®é›†"""
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ•°æ®é›†
        dataset_ids = []
        for i in range(3):
            dataset_id = create_dataset(
                name=f'Test Dataset {i+1}',
                description=f'Dataset number {i+1}',
                dataset_type='custom',
                file_path=self.test_dataset_dir
            )
            dataset_ids.append(dataset_id)
        
        # è·å–æ‰€æœ‰æ•°æ®é›†
        datasets, total = get_datasets()
        self.assertEqual(total, 3)
        
        # éªŒè¯æ•°æ®é›†å†…å®¹
        dataset_names = [d.name for d in datasets]
        self.assertIn('Test Dataset 1', dataset_names)
        self.assertIn('Test Dataset 2', dataset_names)
        self.assertIn('Test Dataset 3', dataset_names)
    
    def test_update_dataset(self):
        """æµ‹è¯•æ›´æ–°æ•°æ®é›†"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset_id = create_dataset(
            self.test_db_path,
            name='Original Name',
            description='Original description',
            dataset_type='custom',
            file_path=self.test_dataset_dir
        )
        
        # æ›´æ–°æ•°æ®é›†
        update_data = {
            'description': 'Updated description',
            'image_count': 10,
            'class_count': 5,
            'annotation_format': 'coco'
        }
        
        success = update_dataset(self.test_db_path, dataset_id, **update_data)
        self.assertTrue(success)
        
        # éªŒè¯æ›´æ–°
        updated_dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertEqual(updated_dataset.description, 'Updated description')
        self.assertEqual(updated_dataset.image_count, 10)
        self.assertEqual(updated_dataset.class_count, 5)
        self.assertEqual(updated_dataset.annotation_format, 'coco')
        # åç§°åº”è¯¥ä¿æŒä¸å˜
        self.assertEqual(updated_dataset.name, 'Original Name')
    
    def test_delete_dataset(self):
        """æµ‹è¯•åˆ é™¤æ•°æ®é›†"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset_id = create_dataset(
            self.test_db_path,
            name='Dataset to Delete',
            description='This dataset will be deleted',
            dataset_type='custom',
            file_path=self.test_dataset_dir
        )
        
        # éªŒè¯æ•°æ®é›†å­˜åœ¨
        dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertIsNotNone(dataset)
        
        # åˆ é™¤æ•°æ®é›†
        success = delete_dataset(self.test_db_path, dataset_id)
        self.assertTrue(success)
        
        # éªŒè¯æ•°æ®é›†å·²åˆ é™¤
        deleted_dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertIsNone(deleted_dataset)
    
    def test_search_datasets(self):
        """æµ‹è¯•æœç´¢æ•°æ®é›†"""
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ•°æ®é›†
        datasets_data = [
            {'name': 'COCO Dataset', 'description': 'Common Objects in Context'},
            {'name': 'ImageNet Dataset', 'description': 'Large scale image recognition'},
            {'name': 'Custom Cat Dataset', 'description': 'Custom dataset with cat images'},
            {'name': 'Dog Classification', 'description': 'Dataset for dog breed classification'}
        ]
        
        for data in datasets_data:
            create_dataset(
                self.test_db_path,
                name=data['name'],
                description=data['description'],
                dataset_type='custom',
                file_path=self.test_dataset_dir
            )
        
        # æµ‹è¯•æŒ‰åç§°æœç´¢
        results = search_datasets(self.test_db_path, 'COCO')
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].name, 'COCO Dataset')
        
        # æµ‹è¯•æŒ‰æè¿°æœç´¢
        results = search_datasets(self.test_db_path, 'classification')
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].name, 'Dog Classification')
        
        # æµ‹è¯•éƒ¨åˆ†åŒ¹é…
        results = search_datasets(self.test_db_path, 'Dataset')
        self.assertEqual(len(results), 3)  # COCO Dataset, ImageNet Dataset, Custom Cat Dataset
        
        # æµ‹è¯•ä¸å­˜åœ¨çš„æœç´¢
        results = search_datasets(self.test_db_path, 'nonexistent')
        self.assertEqual(len(results), 0)
    
    def test_dataset_classes_management(self):
        """æµ‹è¯•æ•°æ®é›†ç±»åˆ«ç®¡ç†"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset_id = create_dataset(
            self.test_db_path,
            name='Dataset with Classes',
            description='Dataset for testing classes',
            dataset_type='custom',
            file_path=self.test_dataset_dir
        )
        
        # æ·»åŠ ç±»åˆ«
        class_names = ['cat', 'dog', 'bird', 'fish']
        for class_name in class_names:
            success = add_dataset_class(self.test_db_path, dataset_id, class_name)
            self.assertTrue(success)
        
        # è·å–ç±»åˆ«
        classes = get_dataset_classes(self.test_db_path, dataset_id)
        self.assertEqual(len(classes), 4)
        self.assertEqual(set(classes), set(class_names))
        
        # åˆ é™¤ç±»åˆ«
        success = remove_dataset_class(self.test_db_path, dataset_id, 'fish')
        self.assertTrue(success)
        
        # éªŒè¯åˆ é™¤
        classes = get_dataset_classes(self.test_db_path, dataset_id)
        self.assertEqual(len(classes), 3)
        self.assertNotIn('fish', classes)
        self.assertIn('cat', classes)
        self.assertIn('dog', classes)
        self.assertIn('bird', classes)
    
    def test_project_dataset_linking(self):
        """æµ‹è¯•é¡¹ç›®æ•°æ®é›†å…³è”"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset_id = create_dataset(
            self.test_db_path,
            name='Project Dataset',
            description='Dataset for project linking test',
            dataset_type='custom',
            file_path=self.test_dataset_dir
        )
        
        project_name = 'test_project'
        
        # å…³è”é¡¹ç›®å’Œæ•°æ®é›†
        success = link_dataset_to_project(self.test_db_path, dataset_id, project_name)
        self.assertTrue(success)
        
        # è·å–é¡¹ç›®çš„æ•°æ®é›†
        project_datasets = get_project_datasets(self.test_db_path, project_name)
        self.assertEqual(len(project_datasets), 1)
        self.assertEqual(project_datasets[0].dataset_id, dataset_id)
        
        # è§£é™¤å…³è”
        success = unlink_dataset_from_project(self.test_db_path, dataset_id, project_name)
        self.assertTrue(success)
        
        # éªŒè¯è§£é™¤å…³è”
        project_datasets = get_project_datasets(self.test_db_path, project_name)
        self.assertEqual(len(project_datasets), 0)
    
    def test_duplicate_dataset_name(self):
        """æµ‹è¯•é‡å¤æ•°æ®é›†åç§°å¤„ç†"""
        # åˆ›å»ºç¬¬ä¸€ä¸ªæ•°æ®é›†
        dataset_id1 = create_dataset(
            self.test_db_path,
            name='Duplicate Name',
            description='First dataset',
            dataset_type='custom',
            file_path=self.test_dataset_dir
        )
        self.assertIsNotNone(dataset_id1)
        
        # å°è¯•åˆ›å»ºåŒåæ•°æ®é›†ï¼Œåº”è¯¥å¤±è´¥
        with self.assertRaises(Exception):
            create_dataset(
                self.test_db_path,
                name='Duplicate Name',
                description='Second dataset',
                dataset_type='custom',
                file_path=self.test_dataset_dir
            )
    
    def test_dataset_with_special_characters(self):
        """æµ‹è¯•åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ•°æ®é›†"""
        special_name = "æ•°æ®é›† with ç‰¹æ®Šå­—ç¬¦ & symbols!"
        special_description = "æè¿° with Ã©mojis ğŸš€ and symbols @#$%"
        
        dataset_id = create_dataset(
            self.test_db_path,
            name=special_name,
            description=special_description,
            dataset_type='custom',
            file_path=self.test_dataset_dir
        )
        
        self.assertIsNotNone(dataset_id)
        
        # éªŒè¯ç‰¹æ®Šå­—ç¬¦æ­£ç¡®ä¿å­˜
        dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertEqual(dataset.name, special_name)
        self.assertEqual(dataset.description, special_description)
    
    def test_large_dataset_values(self):
        """æµ‹è¯•å¤§æ•°å€¼çš„æ•°æ®é›†"""
        large_size = 1024 * 1024 * 1024 * 50  # 50GB
        large_image_count = 1000000  # 100ä¸‡å›¾ç‰‡
        
        dataset_id = create_dataset(
            self.test_db_path,
            name='Large Dataset',
            description='Dataset with large values',
            dataset_type='downloaded',
            file_path=self.test_dataset_dir,
            file_size=large_size,
            image_count=large_image_count
        )
        
        self.assertIsNotNone(dataset_id)
        
        dataset = get_dataset_by_id(self.test_db_path, dataset_id)
        self.assertEqual(dataset.file_size, large_size)
        self.assertEqual(dataset.image_count, large_image_count)


if __name__ == '__main__':
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestDatasetModel)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print(f"\n{'='*50}")
    print(f"æµ‹è¯•å®Œæˆ: è¿è¡Œ {result.testsRun} ä¸ªæµ‹è¯•")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print(f"\nå¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback.split('AssertionError: ')[-1].split(chr(10))[0]}")
    
    if result.errors:
        print(f"\né”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback.split(chr(10))[-2]}")
    
    success = len(result.failures) == 0 and len(result.errors) == 0
    print(f"\næ€»ä½“ç»“æœ: {'é€šè¿‡' if success else 'å¤±è´¥'}")